# Choice of inference framework

To execute the models an inference framework is needed.  
In this decisions I list and evaluate different frameworks.

## Assumptions

-   I need GPU support, otherwise distributed inference is not comparable to local inference. CPU support is optional (although part of most frameworks anyway).

## Solutions

### [onnxruntime-web](https://onnxruntime.ai/docs/tutorials/web/)

Advantages:

-   Easy to use.
-   Supports onnx, which is nice to use.
-   WebGPU and WebNN support.

Disadvantages:

-   Full access to GPU memory is difficult. This might be necessary to run models efficiently (need to keep models in memory, keep kv, update only parts of GPU memory)
-   General purpose. This means it's not tuned for LLMs and therefore slower.

See [code](../../foundation/onnxruntime_web_browser_execution/).

### [webllm](https://github.com/mlc-ai/web-llm) or [mlc-llm](https://github.com/mlc-ai/mlc-llm)

Advantages:

-   Fast since it's optimized for LLMs (~80% native speed)

Disadvantages:

-   Not sure if it supports raw outputs (required for split model)

See [code](../../foundation/webllm_browser_execution/).

### [tvm](https://github.com/apache/tvm/)

Instead of using webllm which uses tvm under the hood I could use tvm directly.

Advantages:

-   Same speed as webllm
-   Customizable

Disadvantages:

-   More complex to set up than webllm
-   Need to compile and set up abstractions myself
-   Not clear whether it supports multiple outputs (might be necessary to pass values along when splitting)

### [Transformers.js](https://huggingface.co/docs/transformers.js/en/index)

### custom framework

Advantages:

-   Fully customizable
-   A lot of learning experience

Disadvantages:

-   Takes time since complex
-   Slower since I'm likely not able to implement many optimizations
-   Limited to simple models/architectures

## Decision

TDB

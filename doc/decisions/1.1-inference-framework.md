# Choice of inference framework

To execute the models an inference framework is needed.  
In this decisions I list and evaluate different frameworks.

## Assumptions

-   I need GPU support, otherwise distributed inference is not comparable to local inference. CPU support is optional (although part of most frameworks anyway).
-   Learning experience is part of the decision. Even if one options fits quite well, if the second makes sense for learning as part of the thesis I might choose it.

## Solutions

### [onnxruntime-web](https://onnxruntime.ai/docs/tutorials/web/)

Advantages:

-   Flexibility with model structures. Custom architectures are nice for split models.
-   Explicit control over inputs/outputs which is to pass (potentially multiple) results between nodes.
-   ONNX has a broad ecosystem and is easy to use.
-   WebGPU and WebNN support.

Disadvantages:

-   General purpose. This means it is likely slow for LLMs.
-   Memory management. Full access to GPU memory is difficult. This might be necessary to run models efficiently (need to keep models in memory, keep kv, update only parts of GPU memory)
-   Currently limited to models less than 4GB.

See [code](../../foundation/onnxruntime_web_browser_execution/).

### [webllm](https://github.com/mlc-ai/web-llm) or [mlc-llm](https://github.com/mlc-ai/mlc-llm)

Advantages:

-   Fast since it's optimized for LLMs (~80% native speed).
-   Easy to use.
-   Optimizations for WebGPU using TVM.

Disadvantages:

-   Not sure if it supports raw outputs (required for split model).
-   Custom architecture might be difficult to use.

See [code](../../foundation/webllm_browser_execution/).

### [tvm](https://github.com/apache/tvm/)

Instead of using webllm which uses tvm under the hood I could use tvm directly.

Advantages:

-   Complete control over computation graph.
-   Same optimization benefits as WebLLM.
-   Flexibility to implement custom operations.

Disadvantages:

-   Steep learning curve.
-   Development overhead: Need to compile and set up abstractions myself.
-   Not clear whether it supports multiple outputs (might be necessary to pass values along when splitting)

### [Transformers.js](https://huggingface.co/docs/transformers.js/en/index)

Uses ONNX Runtime under the hood.

Advantages:

-   Familiar API.
-   Seems to support a large variety of models already.

Disadvantages:

-   Slower than WebLLM? (requires testing; how good is TVM for WebGPU vs. ONNX runtime)
-   Might not support model splitting? (requires testing)
-   Maturity?
-   Control over low level optimization?

### Custom Framework

Advantages:

-   Maximum learning opportunity.
-   No constraints from existing frameworks.

Disadvantages:

-   Significant time investment.
-   Performance challenges: Difficult to match optimization level of specialized frameworks.
-   Limited to simple models/architectures.
-   Risk of encountering fundamental issue: Only choose this variant after detailed investigation of feasibility.
-   Limits scope of thesis: Less time for other parts of the thesis.

## Decision

TBD

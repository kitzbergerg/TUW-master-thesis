# Choice of inference framework

To execute the models an inference framework is needed.  
In this decisions I list and evaluate different frameworks.

## Assumptions

-   I need GPU support, otherwise distributed inference is not comparable to local inference. CPU support is optional (although part of most frameworks anyway).

## Solutions

### [onnxruntime-web](https://onnxruntime.ai/docs/tutorials/web/)

Advantages:

-   Easy to use.
-   Supports onnx, which is nice to use.
-   WebGPU and WebNN support.

Disadvantages:

-   Full access to GPU memory is difficult. This might be necessary to run models efficiently (need to keep models in memory, keep kv, update only parts of GPU memory)
-   General purpose. This means it's not tuned for LLMs and probably slower.

See [code](../../foundation/onnxruntime_web_browser_execution/).

### [webllm](https://github.com/mlc-ai/web-llm)

### [Transformers.js](https://huggingface.co/docs/transformers.js/en/index)

### custom framework

Advantages:

-   Fully customizable
-   A lot of learning experience

Disadvantages:

-   Takes time since complex
-   Might not be as fast as optimized versions (at least not without a lot of time investment)
-   Might be limited to simple models/architectures
-   scope?

## Decision

TDB
